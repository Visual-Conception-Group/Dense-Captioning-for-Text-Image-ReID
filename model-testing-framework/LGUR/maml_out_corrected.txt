Namespace(mode='train', trained=False, bidirectional=True, using_pose=False, last_lstm=False, using_noun=False, epoch=60, start_epoch=0, epoch_decay=[20, 40, 50], wd=4e-05, batch_size=64, adam_alpha=0.9, adam_beta=0.999, lr=0.001, margin=0.3, vocab_size=4750, feature_length=512, class_num=20102, part=6, caption_length_max=100, random_erasing=0.0, Save_param_every=5, save_path='./checkpoints/dual_modal/maml/model_get', GPU_id='7', device=device(type='cuda', index=7), dataset='maml', dataroot='', pkl_root='/home/zefeng/Exp/code/text-image/code by myself/data/processed_data/', test_image_num=200, data_augment=False, d_model=1024, nhead=4, dim_feedforward=2048, normalize_before=False, num_encoder_layers=3, num_decoder_layers=3, num_query=6, detr_lr=0.0001, txt_detr_lr=0.0001, txt_lstm_lr=0.001, res_y=False, noself=False, post_norm=False, n_heads=4, n_layers=2, share_query=True, ViT_layer=8, wordtype='bert')
/raid/home/vibhu20150/anaconda3/envs/lgur/lib/python3.10/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model_size: 212.67774M
/raid/home/vibhu20150/anaconda3/envs/lgur/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
beta lr:0.0001
beta lr:0.0001
beta lr:0.001
beta lr:0.001
beta lr:0.001
alpha lr:0.001
dict_keys(['id', 'img_path', 'same_id_index', 'lstm_caption_id', 'bert_caption_id', 'captions'])
ZURU_Combined-train has 35000 pohtos
dict_keys(['id', 'img_path', 'same_id_index', 'lstm_caption_id', 'bert_caption_id', 'captions'])
ICFG-PEDES-val-train has 31463 pohtos
combined_datasets-val has 9211 pohtos, 9211 text
using stride: [16, 16], and patch number is num_y24 * num_x8
using drop_out rate is : 0.0
using attn_drop_out rate is : 0.0
using drop_path rate is : 0.1
distill need to choose right cls token in the pth
Resized position embedding from size:torch.Size([1, 197, 768]) to size: torch.Size([1, 193, 768]) with height:24 width: 8
Loading pretrained ImageNet model......from ./deit_base_distilled_patch16_224-df68dfff.pth
Loaded model successfully!
Epoch: 1/60 Setp: 50, ranking_loss: 48.33, id_loss: 43.61, ranking_loss_dict: 49.83, id_loss_dict: 44.36,ranking_loss_dict_text: 35.71, ranking_loss_dict_image: 35.59,pred_i2t_local: 0.000 pred_t2i_local 0.000
Epoch: 1/60 Setp: 100, ranking_loss: 39.42, id_loss: 41.23, ranking_loss_dict: 48.46, id_loss_dict: 41.77,ranking_loss_dict_text: 31.56, ranking_loss_dict_image: 24.74,pred_i2t_local: 0.000 pred_t2i_local 0.000
Epoch: 1/60 Setp: 150, ranking_loss: 31.64, id_loss: 40.76, ranking_loss_dict: 49.14, id_loss_dict: 40.26,ranking_loss_dict_text: 32.60, ranking_loss_dict_image: 16.01,pred_i2t_local: 0.000 pred_t2i_local 0.000
Epoch: 1/60 Setp: 200, ranking_loss: 31.26, id_loss: 40.99, ranking_loss_dict: 46.24, id_loss_dict: 41.09,ranking_loss_dict_text: 28.64, ranking_loss_dict_image: 14.15,pred_i2t_local: 0.000 pred_t2i_local 0.000
Epoch: 1/60 Setp: 250, ranking_loss: 30.95, id_loss: 39.02, ranking_loss_dict: 47.06, id_loss_dict: 39.09,ranking_loss_dict_text: 28.51, ranking_loss_dict_image: 11.40,pred_i2t_local: 0.000 pred_t2i_local 0.000
Epoch: 1/60 Setp: 300, ranking_loss: 29.81, id_loss: 40.82, ranking_loss_dict: 44.52, id_loss_dict: 40.97,ranking_loss_dict_text: 28.91, ranking_loss_dict_image: 9.25,pred_i2t_local: 0.000 pred_t2i_local 0.000
Epoch: 1/60 Setp: 350, ranking_loss: 32.01, id_loss: 40.42, ranking_loss_dict: 49.02, id_loss_dict: 39.81,ranking_loss_dict_text: 25.87, ranking_loss_dict_image: 9.23,pred_i2t_local: 0.000 pred_t2i_local 0.000
Epoch: 1/60 Setp: 400, ranking_loss: 24.66, id_loss: 40.28, ranking_loss_dict: 44.90, id_loss_dict: 40.04,ranking_loss_dict_text: 24.34, ranking_loss_dict_image: 5.86,pred_i2t_local: 0.000 pred_t2i_local 0.000
Epoch: 1/60 Setp: 450, ranking_loss: 24.66, id_loss: 40.29, ranking_loss_dict: 47.60, id_loss_dict: 39.79,ranking_loss_dict_text: 27.35, ranking_loss_dict_image: 7.00,pred_i2t_local: 0.000 pred_t2i_local 0.000
Traceback (most recent call last):
  File "/raid/home/vibhu20150/Person-Re-ID/LGUR/maml_temp_ninju.py", line 336, in <module>
    maml_loss.backward()
  File "/raid/home/vibhu20150/anaconda3/envs/lgur/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/raid/home/vibhu20150/anaconda3/envs/lgur/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
