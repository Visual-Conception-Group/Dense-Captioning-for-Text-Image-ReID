dict_keys(['id', 'img_path', 'same_id_index', 'lstm_caption_id', 'bert_caption_id', 'captions'])
ZURU-train has 30000 pohtos
ZURU-test has 10000 pohtos, 10000 text
using stride: [16, 16], and patch number is num_y24 * num_x8
using drop_out rate is : 0.0
using attn_drop_out rate is : 0.0
using drop_path rate is : 0.1
===========================ERROR=========================
shape do not match in k :cls_token: param_dicttorch.Size([1, 1, 768]) vs self.state_dict()torch.Size([1, 1, 384])
distill need to choose right cls token in the pth
Resized position embedding from size:torch.Size([1, 197, 768]) to size: torch.Size([1, 193, 384]) with height:24 width: 8
===========================ERROR=========================
shape do not match in k :pos_embed: param_dicttorch.Size([1, 193, 768]) vs self.state_dict()torch.Size([1, 193, 384])
===========================ERROR=========================
shape do not match in k :patch_embed.proj.weight: param_dicttorch.Size([768, 3, 16, 16]) vs self.state_dict()torch.Size([384, 3, 16, 16])
===========================ERROR=========================
shape do not match in k :patch_embed.proj.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.0.norm1.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.0.norm1.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.0.attn.qkv.weight: param_dicttorch.Size([2304, 768]) vs self.state_dict()torch.Size([1152, 384])
===========================ERROR=========================
shape do not match in k :blocks.0.attn.qkv.bias: param_dicttorch.Size([2304]) vs self.state_dict()torch.Size([1152])
===========================ERROR=========================
shape do not match in k :blocks.0.attn.proj.weight: param_dicttorch.Size([768, 768]) vs self.state_dict()torch.Size([384, 384])
===========================ERROR=========================
shape do not match in k :blocks.0.attn.proj.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.0.norm2.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.0.norm2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.0.mlp.fc1.weight: param_dicttorch.Size([3072, 768]) vs self.state_dict()torch.Size([1536, 384])
===========================ERROR=========================
shape do not match in k :blocks.0.mlp.fc1.bias: param_dicttorch.Size([3072]) vs self.state_dict()torch.Size([1536])
===========================ERROR=========================
shape do not match in k :blocks.0.mlp.fc2.weight: param_dicttorch.Size([768, 3072]) vs self.state_dict()torch.Size([384, 1536])
===========================ERROR=========================
shape do not match in k :blocks.0.mlp.fc2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.1.norm1.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.1.norm1.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.1.attn.qkv.weight: param_dicttorch.Size([2304, 768]) vs self.state_dict()torch.Size([1152, 384])
===========================ERROR=========================
shape do not match in k :blocks.1.attn.qkv.bias: param_dicttorch.Size([2304]) vs self.state_dict()torch.Size([1152])
===========================ERROR=========================
shape do not match in k :blocks.1.attn.proj.weight: param_dicttorch.Size([768, 768]) vs self.state_dict()torch.Size([384, 384])
===========================ERROR=========================
shape do not match in k :blocks.1.attn.proj.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.1.norm2.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.1.norm2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.1.mlp.fc1.weight: param_dicttorch.Size([3072, 768]) vs self.state_dict()torch.Size([1536, 384])
===========================ERROR=========================
shape do not match in k :blocks.1.mlp.fc1.bias: param_dicttorch.Size([3072]) vs self.state_dict()torch.Size([1536])
===========================ERROR=========================
shape do not match in k :blocks.1.mlp.fc2.weight: param_dicttorch.Size([768, 3072]) vs self.state_dict()torch.Size([384, 1536])
===========================ERROR=========================
shape do not match in k :blocks.1.mlp.fc2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.2.norm1.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.2.norm1.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.2.attn.qkv.weight: param_dicttorch.Size([2304, 768]) vs self.state_dict()torch.Size([1152, 384])
===========================ERROR=========================
shape do not match in k :blocks.2.attn.qkv.bias: param_dicttorch.Size([2304]) vs self.state_dict()torch.Size([1152])
===========================ERROR=========================
shape do not match in k :blocks.2.attn.proj.weight: param_dicttorch.Size([768, 768]) vs self.state_dict()torch.Size([384, 384])
===========================ERROR=========================
shape do not match in k :blocks.2.attn.proj.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.2.norm2.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.2.norm2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.2.mlp.fc1.weight: param_dicttorch.Size([3072, 768]) vs self.state_dict()torch.Size([1536, 384])
===========================ERROR=========================
shape do not match in k :blocks.2.mlp.fc1.bias: param_dicttorch.Size([3072]) vs self.state_dict()torch.Size([1536])
===========================ERROR=========================
shape do not match in k :blocks.2.mlp.fc2.weight: param_dicttorch.Size([768, 3072]) vs self.state_dict()torch.Size([384, 1536])
===========================ERROR=========================
shape do not match in k :blocks.2.mlp.fc2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.3.norm1.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.3.norm1.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.3.attn.qkv.weight: param_dicttorch.Size([2304, 768]) vs self.state_dict()torch.Size([1152, 384])
===========================ERROR=========================
shape do not match in k :blocks.3.attn.qkv.bias: param_dicttorch.Size([2304]) vs self.state_dict()torch.Size([1152])
===========================ERROR=========================
shape do not match in k :blocks.3.attn.proj.weight: param_dicttorch.Size([768, 768]) vs self.state_dict()torch.Size([384, 384])
===========================ERROR=========================
shape do not match in k :blocks.3.attn.proj.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.3.norm2.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.3.norm2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.3.mlp.fc1.weight: param_dicttorch.Size([3072, 768]) vs self.state_dict()torch.Size([1536, 384])
===========================ERROR=========================
shape do not match in k :blocks.3.mlp.fc1.bias: param_dicttorch.Size([3072]) vs self.state_dict()torch.Size([1536])
===========================ERROR=========================
shape do not match in k :blocks.3.mlp.fc2.weight: param_dicttorch.Size([768, 3072]) vs self.state_dict()torch.Size([384, 1536])
===========================ERROR=========================
shape do not match in k :blocks.3.mlp.fc2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.4.norm1.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.4.norm1.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.4.attn.qkv.weight: param_dicttorch.Size([2304, 768]) vs self.state_dict()torch.Size([1152, 384])
===========================ERROR=========================
shape do not match in k :blocks.4.attn.qkv.bias: param_dicttorch.Size([2304]) vs self.state_dict()torch.Size([1152])
===========================ERROR=========================
shape do not match in k :blocks.4.attn.proj.weight: param_dicttorch.Size([768, 768]) vs self.state_dict()torch.Size([384, 384])
===========================ERROR=========================
shape do not match in k :blocks.4.attn.proj.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.4.norm2.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.4.norm2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.4.mlp.fc1.weight: param_dicttorch.Size([3072, 768]) vs self.state_dict()torch.Size([1536, 384])
===========================ERROR=========================
shape do not match in k :blocks.4.mlp.fc1.bias: param_dicttorch.Size([3072]) vs self.state_dict()torch.Size([1536])
===========================ERROR=========================
shape do not match in k :blocks.4.mlp.fc2.weight: param_dicttorch.Size([768, 3072]) vs self.state_dict()torch.Size([384, 1536])
===========================ERROR=========================
shape do not match in k :blocks.4.mlp.fc2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.5.norm1.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.5.norm1.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.5.attn.qkv.weight: param_dicttorch.Size([2304, 768]) vs self.state_dict()torch.Size([1152, 384])
===========================ERROR=========================
shape do not match in k :blocks.5.attn.qkv.bias: param_dicttorch.Size([2304]) vs self.state_dict()torch.Size([1152])
===========================ERROR=========================
shape do not match in k :blocks.5.attn.proj.weight: param_dicttorch.Size([768, 768]) vs self.state_dict()torch.Size([384, 384])
===========================ERROR=========================
shape do not match in k :blocks.5.attn.proj.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.5.norm2.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.5.norm2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.5.mlp.fc1.weight: param_dicttorch.Size([3072, 768]) vs self.state_dict()torch.Size([1536, 384])
===========================ERROR=========================
shape do not match in k :blocks.5.mlp.fc1.bias: param_dicttorch.Size([3072]) vs self.state_dict()torch.Size([1536])
===========================ERROR=========================
shape do not match in k :blocks.5.mlp.fc2.weight: param_dicttorch.Size([768, 3072]) vs self.state_dict()torch.Size([384, 1536])
===========================ERROR=========================
shape do not match in k :blocks.5.mlp.fc2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.6.norm1.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.6.norm1.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.6.attn.qkv.weight: param_dicttorch.Size([2304, 768]) vs self.state_dict()torch.Size([1152, 384])
===========================ERROR=========================
shape do not match in k :blocks.6.attn.qkv.bias: param_dicttorch.Size([2304]) vs self.state_dict()torch.Size([1152])
===========================ERROR=========================
shape do not match in k :blocks.6.attn.proj.weight: param_dicttorch.Size([768, 768]) vs self.state_dict()torch.Size([384, 384])
===========================ERROR=========================
shape do not match in k :blocks.6.attn.proj.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.6.norm2.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.6.norm2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.6.mlp.fc1.weight: param_dicttorch.Size([3072, 768]) vs self.state_dict()torch.Size([1536, 384])
===========================ERROR=========================
shape do not match in k :blocks.6.mlp.fc1.bias: param_dicttorch.Size([3072]) vs self.state_dict()torch.Size([1536])
===========================ERROR=========================
shape do not match in k :blocks.6.mlp.fc2.weight: param_dicttorch.Size([768, 3072]) vs self.state_dict()torch.Size([384, 1536])
===========================ERROR=========================
shape do not match in k :blocks.6.mlp.fc2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.7.norm1.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.7.norm1.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.7.attn.qkv.weight: param_dicttorch.Size([2304, 768]) vs self.state_dict()torch.Size([1152, 384])
===========================ERROR=========================
shape do not match in k :blocks.7.attn.qkv.bias: param_dicttorch.Size([2304]) vs self.state_dict()torch.Size([1152])
===========================ERROR=========================
shape do not match in k :blocks.7.attn.proj.weight: param_dicttorch.Size([768, 768]) vs self.state_dict()torch.Size([384, 384])
===========================ERROR=========================
shape do not match in k :blocks.7.attn.proj.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.7.norm2.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.7.norm2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.7.mlp.fc1.weight: param_dicttorch.Size([3072, 768]) vs self.state_dict()torch.Size([1536, 384])
===========================ERROR=========================
shape do not match in k :blocks.7.mlp.fc1.bias: param_dicttorch.Size([3072]) vs self.state_dict()torch.Size([1536])
===========================ERROR=========================
shape do not match in k :blocks.7.mlp.fc2.weight: param_dicttorch.Size([768, 3072]) vs self.state_dict()torch.Size([384, 1536])
===========================ERROR=========================
shape do not match in k :blocks.7.mlp.fc2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.8.norm1.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.8.norm1.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.8.attn.qkv.weight: param_dicttorch.Size([2304, 768]) vs self.state_dict()torch.Size([1152, 384])
===========================ERROR=========================
shape do not match in k :blocks.8.attn.qkv.bias: param_dicttorch.Size([2304]) vs self.state_dict()torch.Size([1152])
===========================ERROR=========================
shape do not match in k :blocks.8.attn.proj.weight: param_dicttorch.Size([768, 768]) vs self.state_dict()torch.Size([384, 384])
===========================ERROR=========================
shape do not match in k :blocks.8.attn.proj.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.8.norm2.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.8.norm2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.8.mlp.fc1.weight: param_dicttorch.Size([3072, 768]) vs self.state_dict()torch.Size([1536, 384])
===========================ERROR=========================
shape do not match in k :blocks.8.mlp.fc1.bias: param_dicttorch.Size([3072]) vs self.state_dict()torch.Size([1536])
===========================ERROR=========================
shape do not match in k :blocks.8.mlp.fc2.weight: param_dicttorch.Size([768, 3072]) vs self.state_dict()torch.Size([384, 1536])
===========================ERROR=========================
shape do not match in k :blocks.8.mlp.fc2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.9.norm1.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.9.norm1.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.9.attn.qkv.weight: param_dicttorch.Size([2304, 768]) vs self.state_dict()torch.Size([1152, 384])
===========================ERROR=========================
shape do not match in k :blocks.9.attn.qkv.bias: param_dicttorch.Size([2304]) vs self.state_dict()torch.Size([1152])
===========================ERROR=========================
shape do not match in k :blocks.9.attn.proj.weight: param_dicttorch.Size([768, 768]) vs self.state_dict()torch.Size([384, 384])
===========================ERROR=========================
shape do not match in k :blocks.9.attn.proj.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.9.norm2.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.9.norm2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.9.mlp.fc1.weight: param_dicttorch.Size([3072, 768]) vs self.state_dict()torch.Size([1536, 384])
===========================ERROR=========================
shape do not match in k :blocks.9.mlp.fc1.bias: param_dicttorch.Size([3072]) vs self.state_dict()torch.Size([1536])
===========================ERROR=========================
shape do not match in k :blocks.9.mlp.fc2.weight: param_dicttorch.Size([768, 3072]) vs self.state_dict()torch.Size([384, 1536])
===========================ERROR=========================
shape do not match in k :blocks.9.mlp.fc2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.10.norm1.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.10.norm1.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.10.attn.qkv.weight: param_dicttorch.Size([2304, 768]) vs self.state_dict()torch.Size([1152, 384])
===========================ERROR=========================
shape do not match in k :blocks.10.attn.qkv.bias: param_dicttorch.Size([2304]) vs self.state_dict()torch.Size([1152])
===========================ERROR=========================
shape do not match in k :blocks.10.attn.proj.weight: param_dicttorch.Size([768, 768]) vs self.state_dict()torch.Size([384, 384])
===========================ERROR=========================
shape do not match in k :blocks.10.attn.proj.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.10.norm2.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.10.norm2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.10.mlp.fc1.weight: param_dicttorch.Size([3072, 768]) vs self.state_dict()torch.Size([1536, 384])
===========================ERROR=========================
shape do not match in k :blocks.10.mlp.fc1.bias: param_dicttorch.Size([3072]) vs self.state_dict()torch.Size([1536])
===========================ERROR=========================
shape do not match in k :blocks.10.mlp.fc2.weight: param_dicttorch.Size([768, 3072]) vs self.state_dict()torch.Size([384, 1536])
===========================ERROR=========================
shape do not match in k :blocks.10.mlp.fc2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.11.norm1.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.11.norm1.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.11.attn.qkv.weight: param_dicttorch.Size([2304, 768]) vs self.state_dict()torch.Size([1152, 384])
===========================ERROR=========================
shape do not match in k :blocks.11.attn.qkv.bias: param_dicttorch.Size([2304]) vs self.state_dict()torch.Size([1152])
===========================ERROR=========================
shape do not match in k :blocks.11.attn.proj.weight: param_dicttorch.Size([768, 768]) vs self.state_dict()torch.Size([384, 384])
===========================ERROR=========================
shape do not match in k :blocks.11.attn.proj.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.11.norm2.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.11.norm2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :blocks.11.mlp.fc1.weight: param_dicttorch.Size([3072, 768]) vs self.state_dict()torch.Size([1536, 384])
===========================ERROR=========================
shape do not match in k :blocks.11.mlp.fc1.bias: param_dicttorch.Size([3072]) vs self.state_dict()torch.Size([1536])
===========================ERROR=========================
shape do not match in k :blocks.11.mlp.fc2.weight: param_dicttorch.Size([768, 3072]) vs self.state_dict()torch.Size([384, 1536])
===========================ERROR=========================
shape do not match in k :blocks.11.mlp.fc2.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :norm.weight: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
===========================ERROR=========================
shape do not match in k :norm.bias: param_dicttorch.Size([768]) vs self.state_dict()torch.Size([384])
Loading pretrained ImageNet model......from /ssd-scratch/vibhu20150/Person-Re-ID/LGUR/deit_base_distilled_patch16_224-df68dfff.pth
model_get
Testing Epoch: 1 Iteration:468
similarity_all:
t2i: @R1: 0.0031, @R5: 0.0067, @R10: 0.0096, map: 0.007076
t2i_wm: @R1: 0.0031, @R5: 0.0067, @R10: 0.0096, map: 0.00517
model_get
Testing Epoch: 2 Iteration:468
similarity_all:
t2i: @R1: 0.0055, @R5: 0.0129, @R10: 0.0203, map: 0.01228
t2i_wm: @R1: 0.0055, @R5: 0.0129, @R10: 0.0203, map: 0.009057
model_get
Testing Epoch: 3 Iteration:468
similarity_all:
t2i: @R1: 0.0121, @R5: 0.0268, @R10: 0.0419, map: 0.0239
t2i_wm: @R1: 0.0121, @R5: 0.0268, @R10: 0.0419, map: 0.01831
model_get
Testing Epoch: 4 Iteration:468
similarity_all:
t2i: @R1: 0.0183, @R5: 0.0451, @R10: 0.0665, map: 0.03628
t2i_wm: @R1: 0.0183, @R5: 0.0451, @R10: 0.0665, map: 0.02785
model_get
Testing Epoch: 5 Iteration:468
similarity_all:
t2i: @R1: 0.0227, @R5: 0.0544, @R10: 0.08, map: 0.04301
t2i_wm: @R1: 0.0227, @R5: 0.0544, @R10: 0.08, map: 0.0335
model_get
Testing Epoch: 6 Iteration:468
similarity_all:
t2i: @R1: 0.0325, @R5: 0.0772, @R10: 0.1101, map: 0.05798
t2i_wm: @R1: 0.0325, @R5: 0.0772, @R10: 0.1101, map: 0.04614
model_get
Testing Epoch: 7 Iteration:468
similarity_all:
t2i: @R1: 0.0419, @R5: 0.0932, @R10: 0.1307, map: 0.07012
t2i_wm: @R1: 0.0419, @R5: 0.0932, @R10: 0.1307, map: 0.05703
model_get
Testing Epoch: 8 Iteration:468
similarity_all:
t2i: @R1: 0.0479, @R5: 0.1057, @R10: 0.1443, map: 0.0788
t2i_wm: @R1: 0.0479, @R5: 0.1057, @R10: 0.1443, map: 0.06451
model_get
Testing Epoch: 9 Iteration:468
similarity_all:
t2i: @R1: 0.0472, @R5: 0.1056, @R10: 0.1423, map: 0.07862
t2i_wm: @R1: 0.0472, @R5: 0.1056, @R10: 0.1423, map: 0.06409
model_get
Testing Epoch: 10 Iteration:468
similarity_all:
t2i: @R1: 0.0529, @R5: 0.1158, @R10: 0.1552, map: 0.0855
t2i_wm: @R1: 0.0529, @R5: 0.1158, @R10: 0.1552, map: 0.07046
model_get
Testing Epoch: 11 Iteration:468
similarity_all:
t2i: @R1: 0.057, @R5: 0.119, @R10: 0.1617, map: 0.08971
t2i_wm: @R1: 0.057, @R5: 0.119, @R10: 0.1617, map: 0.07459
model_get
Testing Epoch: 12 Iteration:468
similarity_all:
t2i: @R1: 0.0562, @R5: 0.1217, @R10: 0.1667, map: 0.09041
t2i_wm: @R1: 0.0562, @R5: 0.1217, @R10: 0.1667, map: 0.07462
model_get
Testing Epoch: 13 Iteration:468
similarity_all:
t2i: @R1: 0.0599, @R5: 0.1296, @R10: 0.1731, map: 0.09505
t2i_wm: @R1: 0.0599, @R5: 0.1296, @R10: 0.1731, map: 0.07885
model_get
Testing Epoch: 14 Iteration:468
similarity_all:
t2i: @R1: 0.0579, @R5: 0.1249, @R10: 0.1648, map: 0.09157
t2i_wm: @R1: 0.0579, @R5: 0.1249, @R10: 0.1648, map: 0.07607
model_get
Testing Epoch: 15 Iteration:468
similarity_all:
t2i: @R1: 0.0655, @R5: 0.1402, @R10: 0.1866, map: 0.1021
t2i_wm: @R1: 0.0655, @R5: 0.1402, @R10: 0.1866, map: 0.08527
model_get
Testing Epoch: 16 Iteration:468
similarity_all:
t2i: @R1: 0.0617, @R5: 0.1317, @R10: 0.1837, map: 0.09826
t2i_wm: @R1: 0.0617, @R5: 0.1317, @R10: 0.1837, map: 0.08139
model_get
Testing Epoch: 17 Iteration:468
similarity_all:
t2i: @R1: 0.0666, @R5: 0.1374, @R10: 0.1837, map: 0.1023
t2i_wm: @R1: 0.0666, @R5: 0.1374, @R10: 0.1837, map: 0.08585
model_get
Testing Epoch: 18 Iteration:468
similarity_all:
t2i: @R1: 0.0659, @R5: 0.1391, @R10: 0.183, map: 0.1026
t2i_wm: @R1: 0.0659, @R5: 0.1391, @R10: 0.183, map: 0.08571
model_get
Testing Epoch: 19 Iteration:468
similarity_all:
t2i: @R1: 0.0697, @R5: 0.1416, @R10: 0.1896, map: 0.1063
t2i_wm: @R1: 0.0697, @R5: 0.1416, @R10: 0.1896, map: 0.08944
model_get
Testing Epoch: 20 Iteration:468
similarity_all:
t2i: @R1: 0.0792, @R5: 0.1604, @R10: 0.2107, map: 0.119
t2i_wm: @R1: 0.0792, @R5: 0.1604, @R10: 0.2107, map: 0.1007
model_get
Testing Epoch: 21 Iteration:468
similarity_all:
t2i: @R1: 0.0812, @R5: 0.1572, @R10: 0.2089, map: 0.1199
t2i_wm: @R1: 0.0812, @R5: 0.1572, @R10: 0.2089, map: 0.1021
model_get
Testing Epoch: 22 Iteration:468
similarity_all:
t2i: @R1: 0.0806, @R5: 0.1572, @R10: 0.2106, map: 0.1199
t2i_wm: @R1: 0.0806, @R5: 0.1572, @R10: 0.2106, map: 0.1018
model_get
Testing Epoch: 23 Iteration:468
similarity_all:
t2i: @R1: 0.08, @R5: 0.1596, @R10: 0.2103, map: 0.1193
t2i_wm: @R1: 0.08, @R5: 0.1596, @R10: 0.2103, map: 0.1012
model_get
Testing Epoch: 24 Iteration:468
similarity_all:
t2i: @R1: 0.079, @R5: 0.1613, @R10: 0.2121, map: 0.1191
t2i_wm: @R1: 0.079, @R5: 0.1613, @R10: 0.2121, map: 0.1007
model_get
Testing Epoch: 25 Iteration:468
similarity_all:
t2i: @R1: 0.0797, @R5: 0.1602, @R10: 0.2176, map: 0.1202
t2i_wm: @R1: 0.0797, @R5: 0.1602, @R10: 0.2176, map: 0.1016
model_get
Testing Epoch: 26 Iteration:468
similarity_all:
t2i: @R1: 0.0798, @R5: 0.1639, @R10: 0.217, map: 0.12
t2i_wm: @R1: 0.0798, @R5: 0.1639, @R10: 0.217, map: 0.1015
model_get
Testing Epoch: 27 Iteration:468
similarity_all:
t2i: @R1: 0.0824, @R5: 0.1655, @R10: 0.2207, map: 0.1229
t2i_wm: @R1: 0.0824, @R5: 0.1655, @R10: 0.2207, map: 0.1043
model_get
Testing Epoch: 28 Iteration:468
similarity_all:
t2i: @R1: 0.0848, @R5: 0.1683, @R10: 0.2193, map: 0.1251
t2i_wm: @R1: 0.0848, @R5: 0.1683, @R10: 0.2193, map: 0.1065
model_get
Testing Epoch: 29 Iteration:468
similarity_all:
t2i: @R1: 0.0856, @R5: 0.1716, @R10: 0.2238, map: 0.1264
t2i_wm: @R1: 0.0856, @R5: 0.1716, @R10: 0.2238, map: 0.1076
model_get
Testing Epoch: 30 Iteration:468
similarity_all:
t2i: @R1: 0.0843, @R5: 0.1674, @R10: 0.2219, map: 0.125
t2i_wm: @R1: 0.0843, @R5: 0.1674, @R10: 0.2219, map: 0.1063
model_get
Testing Epoch: 31 Iteration:468
similarity_all:
t2i: @R1: 0.084, @R5: 0.164, @R10: 0.2174, map: 0.1235
t2i_wm: @R1: 0.084, @R5: 0.164, @R10: 0.2174, map: 0.1053
model_get
Testing Epoch: 32 Iteration:468
similarity_all:
t2i: @R1: 0.0846, @R5: 0.1682, @R10: 0.2216, map: 0.1254
t2i_wm: @R1: 0.0846, @R5: 0.1682, @R10: 0.2216, map: 0.1067
model_get
Testing Epoch: 33 Iteration:468
similarity_all:
t2i: @R1: 0.0857, @R5: 0.1702, @R10: 0.222, map: 0.1263
t2i_wm: @R1: 0.0857, @R5: 0.1702, @R10: 0.222, map: 0.1077
model_get
Testing Epoch: 34 Iteration:468
similarity_all:
t2i: @R1: 0.085, @R5: 0.1696, @R10: 0.2248, map: 0.1256
t2i_wm: @R1: 0.085, @R5: 0.1696, @R10: 0.2248, map: 0.107
model_get
Testing Epoch: 35 Iteration:468
similarity_all:
t2i: @R1: 0.0858, @R5: 0.1725, @R10: 0.2257, map: 0.1272
t2i_wm: @R1: 0.0858, @R5: 0.1725, @R10: 0.2257, map: 0.1082
model_get
Testing Epoch: 36 Iteration:468
similarity_all:
t2i: @R1: 0.0876, @R5: 0.1725, @R10: 0.2263, map: 0.1286
t2i_wm: @R1: 0.0876, @R5: 0.1725, @R10: 0.2263, map: 0.1098
model_get
Testing Epoch: 37 Iteration:468
similarity_all:
t2i: @R1: 0.0891, @R5: 0.1742, @R10: 0.2268, map: 0.1301
t2i_wm: @R1: 0.0891, @R5: 0.1742, @R10: 0.2268, map: 0.1113
model_get
Testing Epoch: 38 Iteration:468
similarity_all:
t2i: @R1: 0.0894, @R5: 0.1776, @R10: 0.2312, map: 0.1306
t2i_wm: @R1: 0.0894, @R5: 0.1776, @R10: 0.2312, map: 0.1117
model_get
Testing Epoch: 39 Iteration:468
similarity_all:
t2i: @R1: 0.0865, @R5: 0.1717, @R10: 0.2261, map: 0.1275
t2i_wm: @R1: 0.0865, @R5: 0.1717, @R10: 0.2261, map: 0.1087
model_get
Testing Epoch: 40 Iteration:468
similarity_all:
t2i: @R1: 0.0858, @R5: 0.1763, @R10: 0.2289, map: 0.1279
t2i_wm: @R1: 0.0858, @R5: 0.1763, @R10: 0.2289, map: 0.1086
model_get
Testing Epoch: 41 Iteration:468
similarity_all:
t2i: @R1: 0.0882, @R5: 0.1777, @R10: 0.2284, map: 0.1299
t2i_wm: @R1: 0.0882, @R5: 0.1777, @R10: 0.2284, map: 0.1108
model_get
Testing Epoch: 42 Iteration:468
similarity_all:
t2i: @R1: 0.0872, @R5: 0.1758, @R10: 0.2294, map: 0.1291
t2i_wm: @R1: 0.0872, @R5: 0.1758, @R10: 0.2294, map: 0.1099
model_get
Testing Epoch: 43 Iteration:468
similarity_all:
t2i: @R1: 0.0882, @R5: 0.1761, @R10: 0.2287, map: 0.1296
t2i_wm: @R1: 0.0882, @R5: 0.1761, @R10: 0.2287, map: 0.1106
model_get
Testing Epoch: 44 Iteration:468
similarity_all:
t2i: @R1: 0.0887, @R5: 0.1779, @R10: 0.2295, map: 0.1301
t2i_wm: @R1: 0.0887, @R5: 0.1779, @R10: 0.2295, map: 0.1111
model_get
Testing Epoch: 45 Iteration:468
similarity_all:
t2i: @R1: 0.0883, @R5: 0.177, @R10: 0.2299, map: 0.1301
t2i_wm: @R1: 0.0883, @R5: 0.177, @R10: 0.2299, map: 0.111
model_get
Testing Epoch: 46 Iteration:468
similarity_all:
t2i: @R1: 0.0885, @R5: 0.1779, @R10: 0.2298, map: 0.1301
t2i_wm: @R1: 0.0885, @R5: 0.1779, @R10: 0.2298, map: 0.111
model_get
Testing Epoch: 47 Iteration:468
similarity_all:
t2i: @R1: 0.0879, @R5: 0.1773, @R10: 0.2302, map: 0.1297
t2i_wm: @R1: 0.0879, @R5: 0.1773, @R10: 0.2302, map: 0.1106
model_get
Testing Epoch: 48 Iteration:468
similarity_all:
t2i: @R1: 0.0883, @R5: 0.1775, @R10: 0.2298, map: 0.1297
t2i_wm: @R1: 0.0883, @R5: 0.1775, @R10: 0.2298, map: 0.1107
model_get
Testing Epoch: 49 Iteration:468
similarity_all:
t2i: @R1: 0.0892, @R5: 0.1775, @R10: 0.2295, map: 0.1304
t2i_wm: @R1: 0.0892, @R5: 0.1775, @R10: 0.2295, map: 0.1115
model_get
Testing Epoch: 50 Iteration:468
similarity_all:
t2i: @R1: 0.0892, @R5: 0.1778, @R10: 0.2306, map: 0.1307
t2i_wm: @R1: 0.0892, @R5: 0.1778, @R10: 0.2306, map: 0.1116
model_get
Testing Epoch: 51 Iteration:468
similarity_all:
t2i: @R1: 0.0888, @R5: 0.1768, @R10: 0.2311, map: 0.1303
t2i_wm: @R1: 0.0888, @R5: 0.1768, @R10: 0.2311, map: 0.1112
model_get
Testing Epoch: 52 Iteration:468
similarity_all:
t2i: @R1: 0.0892, @R5: 0.1772, @R10: 0.23, map: 0.1307
t2i_wm: @R1: 0.0892, @R5: 0.1772, @R10: 0.23, map: 0.1116
model_get
Testing Epoch: 53 Iteration:468
similarity_all:
t2i: @R1: 0.089, @R5: 0.1775, @R10: 0.2301, map: 0.1305
t2i_wm: @R1: 0.089, @R5: 0.1775, @R10: 0.2301, map: 0.1115
model_get
Testing Epoch: 54 Iteration:468
similarity_all:
t2i: @R1: 0.0886, @R5: 0.1778, @R10: 0.2292, map: 0.1303
t2i_wm: @R1: 0.0886, @R5: 0.1778, @R10: 0.2292, map: 0.1112
model_get
Testing Epoch: 55 Iteration:468
similarity_all:
t2i: @R1: 0.0886, @R5: 0.1782, @R10: 0.2296, map: 0.1304
t2i_wm: @R1: 0.0886, @R5: 0.1782, @R10: 0.2296, map: 0.1112
model_get
Testing Epoch: 56 Iteration:468
similarity_all:
t2i: @R1: 0.0892, @R5: 0.178, @R10: 0.2295, map: 0.1308
t2i_wm: @R1: 0.0892, @R5: 0.178, @R10: 0.2295, map: 0.1117
model_get
Testing Epoch: 57 Iteration:468
similarity_all:
t2i: @R1: 0.0888, @R5: 0.1783, @R10: 0.2294, map: 0.1305
t2i_wm: @R1: 0.0888, @R5: 0.1783, @R10: 0.2294, map: 0.1114
model_get
Testing Epoch: 58 Iteration:468
similarity_all:
t2i: @R1: 0.0894, @R5: 0.1778, @R10: 0.23, map: 0.1309
t2i_wm: @R1: 0.0894, @R5: 0.1778, @R10: 0.23, map: 0.1119
model_get
Testing Epoch: 59 Iteration:468
similarity_all:
t2i: @R1: 0.0887, @R5: 0.1781, @R10: 0.23, map: 0.1304
t2i_wm: @R1: 0.0887, @R5: 0.1781, @R10: 0.23, map: 0.1113
model_get
Testing Epoch: 60 Iteration:468
similarity_all:
t2i: @R1: 0.0885, @R5: 0.1782, @R10: 0.2296, map: 0.1303
t2i_wm: @R1: 0.0885, @R5: 0.1782, @R10: 0.2296, map: 0.1111
